{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1e57d82-c4bd-4411-b6ed-05a093b73d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 -m venv covid-venv\n",
    "!source covid-venv/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38608fde-b081-4baa-ba15-b246ef020743",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "output_dir = 'output/bert'\n",
    "\n",
    "def ensure_output_dir(output_dir):\n",
    "    \"\"\"Create output directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "def plot_roc_curves(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Plot ROC curves for each class\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(\n",
    "            fpr, \n",
    "            tpr, \n",
    "            label=f'{label} (AUC = {roc_auc:.2f})'\n",
    "        )\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'roc_curves.png'))\n",
    "    plt.close()\n",
    "    \n",
    "class MetricTracker:\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "    \n",
    "    def update(self, metrics_dict):\n",
    "        for key, value in metrics_dict.items():\n",
    "            self.metrics[key].append(value)\n",
    "    \n",
    "    def get_metric(self, metric_name):\n",
    "        return self.metrics[metric_name]\n",
    "\n",
    "def plot_training_history(tracker, fold, output_dir):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(tracker.get_metric('train_loss'), label='Train Loss')\n",
    "    plt.plot(tracker.get_metric('val_loss'), label='Validation Loss')\n",
    "    plt.title(f'Loss History - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(tracker.get_metric('exact_match_accuracy'), label='Exact Match')\n",
    "    plt.plot(tracker.get_metric('hamming_accuracy'), label='Hamming')\n",
    "    plt.title(f'Accuracy History - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot F1, Precision, Recall\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(tracker.get_metric('f1'), label='F1')\n",
    "    plt.plot(tracker.get_metric('precision'), label='Precision')\n",
    "    plt.plot(tracker.get_metric('recall'), label='Recall')\n",
    "    plt.title(f'Metrics History - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'training_history_fold_{fold}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "def plot_confusion_matrices(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Plot confusion matrix for each class\"\"\"\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    n_classes = len(labels)\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, label in enumerate(labels):\n",
    "        cm = confusion_matrix(y_true[:, idx], y_pred_binary[:, idx])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Confusion Matrix - {label}')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('True')\n",
    "    \n",
    "    if len(labels) < len(axes):\n",
    "        for idx in range(len(labels), len(axes)):\n",
    "            fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrices.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_label_distribution(train_labels, test_labels, labels, output_dir):\n",
    "    \"\"\"Plot label distribution in train and test sets\"\"\"\n",
    "    train_dist = train_labels.sum(axis=0)\n",
    "    test_dist = test_labels.sum(axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_dist, width, label='Train')\n",
    "    plt.bar(x + width/2, test_dist, width, label='Test')\n",
    "    \n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Label Distribution in Train and Test Sets')\n",
    "    plt.xticks(x, labels, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'label_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "def create_performance_tables(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Create and save detailed performance tables\"\"\"\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': [],\n",
    "        'Support': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true[:, i], y_pred_binary[:, i], average='binary'\n",
    "        )\n",
    "        metrics_dict['Precision'].append(precision)\n",
    "        metrics_dict['Recall'].append(recall)\n",
    "        metrics_dict['F1-Score'].append(f1)\n",
    "        metrics_dict['Support'].append(support)\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics_dict, index=labels)\n",
    "    df_metrics.to_csv(os.path.join(output_dir, 'class_performance_metrics.csv'))\n",
    "    \n",
    "    corr_matrix = np.corrcoef(y_pred_binary.T)\n",
    "    df_corr = pd.DataFrame(corr_matrix, index=labels, columns=labels)\n",
    "    df_corr.to_csv(os.path.join(output_dir, 'prediction_correlations.csv'))\n",
    "    \n",
    "    return df_metrics, df_corr\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize labels as None - will be set later\n",
    "        self.labels = None\n",
    "        \n",
    "    def get_unique_labels(self, train_df):\n",
    "        \"\"\"Extract unique labels from training data\"\"\"\n",
    "        # Get all unique labels by splitting the semicolon-separated values\n",
    "        all_labels = set()\n",
    "        for labels in train_df['label']:\n",
    "            all_labels.update(labels.split(';'))\n",
    "        # Sort for consistency\n",
    "        self.labels = sorted(list(all_labels))\n",
    "        print(f\"Found {len(self.labels)} unique labels: {self.labels}\")\n",
    "        return self.labels\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def process_labels(self, label_text):\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Labels have not been initialized. Call get_unique_labels first.\")\n",
    "            \n",
    "        label_list = label_text.split(';')\n",
    "        label_array = np.zeros(len(self.labels))\n",
    "        for label in label_list:\n",
    "            if label in self.labels:\n",
    "                label_array[self.labels.index(label)] = 1\n",
    "            else:\n",
    "                print(f\"Warning: Unknown label encountered: {label}\")\n",
    "        return label_array\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Tokenize text using BERT tokenizer\"\"\"\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoding\n",
    "    \n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=7):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Define dimensions\n",
    "        self.bert_hidden_size = self.bert.config.hidden_size  # 768\n",
    "        self.hidden_size1 = 512\n",
    "        self.hidden_size2 = 256\n",
    "        \n",
    "        # Layers\n",
    "        self.layer1 = nn.Linear(self.bert_hidden_size, self.hidden_size1)\n",
    "        self.layer2 = nn.Linear(self.hidden_size1, self.hidden_size2)\n",
    "        self.classifier = nn.Linear(self.hidden_size2, num_labels)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()  # Added this line - initialize ReLU\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Additional layers with activation and dropout\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "def plot_metrics_heatmap(metrics_dict, labels, output_dir):\n",
    "    \"\"\"Create a heatmap of metrics for each category\"\"\"\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Precision': metrics_dict['Precision'],\n",
    "        'Recall': metrics_dict['Recall'],\n",
    "        'F1-Score': metrics_dict['F1-Score']\n",
    "    }, index=labels)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(metrics_df, annot=True, cmap='YlOrRd', fmt='.3f')\n",
    "    plt.title('Performance Metrics by Category')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'metrics_heatmap.png'))\n",
    "    plt.close()\n",
    "    \n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate various metrics for multi-label classification\"\"\"\n",
    "    # Convert predictions to binary\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Exact match accuracy (all labels must match)\n",
    "    exact_match_accuracy = np.mean(np.all(y_pred_binary == y_true, axis=1))\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_accuracy = np.mean(y_pred_binary == y_true, axis=0)\n",
    "    \n",
    "    # Hamming accuracy (proportion of correct predictions)\n",
    "    hamming_accuracy = np.mean(y_pred_binary == y_true)\n",
    "    \n",
    "    # Calculate precision, recall, f1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='samples'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'exact_match_accuracy': exact_match_accuracy,\n",
    "        'hamming_accuracy': hamming_accuracy,\n",
    "        'per_class_accuracy': per_class_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def calculate_overall_metrics(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Calculate both overall and per-category metrics\"\"\"\n",
    "    # Verify input dimensions\n",
    "    assert y_true.shape == y_pred.shape, f\"Shape mismatch: y_true {y_true.shape} != y_pred {y_pred.shape}\"\n",
    "    assert y_true.shape[1] == len(labels), f\"Number of labels mismatch: {y_true.shape[1]} != {len(labels)}\"\n",
    "    \n",
    "    # Convert predictions to binary\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Per-category metrics\n",
    "    per_category_metrics = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': [],\n",
    "        'Support': []\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPer-category Metrics:\")\n",
    "    print(\"--------------------\")\n",
    "    for i, label in enumerate(labels):\n",
    "        try:\n",
    "            precision, recall, f1, support = precision_recall_fscore_support(\n",
    "                y_true[:, i], y_pred_binary[:, i], average='binary'\n",
    "            )\n",
    "            per_category_metrics['Precision'].append(precision)\n",
    "            per_category_metrics['Recall'].append(recall)\n",
    "            per_category_metrics['F1-Score'].append(f1)\n",
    "            per_category_metrics['Support'].append(support)\n",
    "            \n",
    "            print(f\"\\n{label}:\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"F1-Score: {f1:.4f}\")\n",
    "            print(f\"Support: {support}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing label {label} at index {i}: {str(e)}\")\n",
    "            print(f\"Label shape: {y_true[:, i].shape}\")\n",
    "            print(f\"Prediction shape: {y_pred_binary[:, i].shape}\")\n",
    "            raise\n",
    "    \n",
    "    # Overall metrics\n",
    "    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='micro'\n",
    "    )\n",
    "    \n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='macro'\n",
    "    )\n",
    "    \n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='weighted'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Exact match ratio (perfect predictions across all categories)\n",
    "    exact_match = np.mean(np.all(y_pred_binary == y_true, axis=1))\n",
    "    \n",
    "    # Hamming accuracy (percentage of correct labels)\n",
    "    hamming_accuracy = np.mean(y_pred_binary == y_true)\n",
    "    \n",
    "    # Create summary dictionary\n",
    "    overall_metrics = {\n",
    "        'Micro-average': {\n",
    "            'Precision': micro_precision,\n",
    "            'Recall': micro_recall,\n",
    "            'F1-Score': micro_f1\n",
    "        },\n",
    "        'Macro-average': {\n",
    "            'Precision': macro_precision,\n",
    "            'Recall': macro_recall,\n",
    "            'F1-Score': macro_f1\n",
    "        },\n",
    "        'Weighted-average': {\n",
    "            'Precision': weighted_precision,\n",
    "            'Recall': weighted_recall,\n",
    "            'F1-Score': weighted_f1\n",
    "        },\n",
    "        'Exact Match Ratio': exact_match,\n",
    "        'Hamming Accuracy': hamming_accuracy\n",
    "    }\n",
    "    \n",
    "    # Create and display summary DataFrame\n",
    "    df_overall = pd.DataFrame({\n",
    "        'Metric': ['Precision', 'Recall', 'F1-Score'],\n",
    "        'Micro-avg': [micro_precision, micro_recall, micro_f1],\n",
    "        'Macro-avg': [macro_precision, macro_recall, macro_f1],\n",
    "        'Weighted-avg': [weighted_precision, weighted_recall, weighted_f1]\n",
    "    }).set_index('Metric')\n",
    "    \n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(\"--------------\")\n",
    "    print(f\"\\nExact Match Ratio: {exact_match:.4f}\")\n",
    "    print(f\"Hamming Accuracy: {hamming_accuracy:.4f}\")\n",
    "    print(\"\\nAveraged Metrics:\")\n",
    "    print(df_overall)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    df_overall.to_csv(os.path.join(output_dir,'overall_metrics.csv'))\n",
    "    df_categories = pd.DataFrame(per_category_metrics, index=labels)\n",
    "    df_categories.to_csv(os.path.join(output_dir,'per_category_metrics.csv'))\n",
    "    \n",
    "    return overall_metrics, per_category_metrics\n",
    "\n",
    "class BERTTrainer:\n",
    "    def __init__(self, model, device, output_dir):\n",
    "        self.model = model.to(device)  # Move model to device immediately\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "        self.tracker = MetricTracker()\n",
    "        \n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        \"\"\"Evaluate model during training\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                # Move batch tensors to device\n",
    "                input_ids = batch['input_ids'].to(trainer.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                predictions = outputs.cpu().numpy()\n",
    "                all_predictions.extend(predictions)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_labels = np.array(all_labels)\n",
    "        metrics = calculate_metrics(all_labels, all_predictions)\n",
    "        \n",
    "        return val_loss/len(loader), metrics\n",
    "    \n",
    "    def evaluate_final(self, loader, labels):\n",
    "        \"\"\"Final evaluation on test set\"\"\"\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc='Testing'):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                batch_labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.criterion(outputs, batch_labels)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                predictions = outputs.cpu().numpy()\n",
    "                all_predictions.extend(predictions)\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        overall_metrics, per_category_metrics = calculate_overall_metrics(\n",
    "            all_labels, all_predictions, labels, self.output_dir\n",
    "        )\n",
    "        \n",
    "        return test_loss/len(loader), all_predictions, all_labels, overall_metrics, per_category_metrics\n",
    "\n",
    "def process_data(df, processor):\n",
    "    \"\"\"\n",
    "    Process dataframe into BERT-ready dataset\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame containing 'abstract' and 'label' columns\n",
    "        processor: DocumentProcessor instance\n",
    "    \n",
    "    Returns:\n",
    "        COVIDDataset instance\n",
    "    \"\"\"\n",
    "    # Clean abstracts\n",
    "    abstracts = df['abstract'].apply(processor.clean_text).values\n",
    "    \n",
    "    # Convert labels to multi-hot encoding\n",
    "    labels = np.array([processor.process_labels(label) for label in df['label']])\n",
    "    \n",
    "    # Tokenize texts\n",
    "    encodings = []\n",
    "    for abstract in tqdm(abstracts, desc=\"Tokenizing texts\"):\n",
    "        encoding = processor.tokenize_text(abstract)\n",
    "        encodings.append({\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        })\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = COVIDDataset(\n",
    "        texts=abstracts,\n",
    "        labels=labels,\n",
    "        processor=processor\n",
    "    )\n",
    "    \n",
    "    print(f\"Processed {len(dataset)} samples\")\n",
    "    return dataset\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, texts, labels, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: list of abstract texts\n",
    "            labels: numpy array of multi-hot encoded labels\n",
    "            processor: DocumentProcessor instance\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.processor.tokenize_text(text)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b3c2392-fed7-4acf-95f4-e1b7814f6301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f68c10-82a6-4767-af5b-5d62b3c50732",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Found 7 unique labels: ['Case Report', 'Diagnosis', 'Epidemic Forecasting', 'Mechanism', 'Prevention', 'Transmission', 'Treatment']\n",
      "Processing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts: 100%|██████████| 24960/24960 [00:46<00:00, 534.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 24960 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts: 100%|██████████| 6239/6239 [00:11<00:00, 530.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6239 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts: 100%|██████████| 2500/2500 [00:04<00:00, 501.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|██████████| 1560/1560 [04:04<00:00,  6.38it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.2342\n",
      "Validation Loss: 0.1398\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7492\n",
      "  Hamming Accuracy: 0.9490\n",
      "  F1: 0.8751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|██████████| 1560/1560 [04:05<00:00,  6.36it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Training Loss: 0.1217\n",
      "Validation Loss: 0.1170\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7726\n",
      "  Hamming Accuracy: 0.9556\n",
      "  F1: 0.8941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|██████████| 1560/1560 [04:05<00:00,  6.36it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Training Loss: 0.0957\n",
      "Validation Loss: 0.1182\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7759\n",
      "  Hamming Accuracy: 0.9570\n",
      "  F1: 0.8998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training: 100%|██████████| 1560/1560 [04:04<00:00,  6.39it/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'output/bert'\n",
    "ensure_output_dir(output_dir)\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv('./dataset/BC7-LitCovid-Train.csv')\n",
    "val_df = pd.read_csv('./dataset/BC7-LitCovid-Dev.csv')\n",
    "test_df = pd.read_csv('./dataset/BC7-LitCovid-Test-GS.csv')\n",
    "\n",
    "train_labels = np.array([processor.process_labels(label) for label in train_df['label']])\n",
    "\n",
    "# Initialize processor\n",
    "processor = DocumentProcessor(model_name=model_name)\n",
    "\n",
    "# Initialize labels using training data\n",
    "processor.get_unique_labels(train_df)\n",
    "\n",
    "# Process data\n",
    "print(\"Processing datasets...\")\n",
    "train_dataset = process_data(train_df, processor)\n",
    "val_dataset = process_data(val_df, processor)\n",
    "test_dataset = process_data(test_df, processor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Setup device and model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BERTClassifier(model_name=model_name, num_labels=len(processor.labels))\n",
    "\n",
    "# Initialize trainer and run training\n",
    "trainer = BERTTrainer(model, device, output_dir)\n",
    "# test_loss, test_predictions, test_labels_array, overall_metrics, per_category_metrics = trainer.train_and_evaluate(train_loader, val_loader, test_loader, processor)\n",
    "total_epochs=5\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(total_epochs):\n",
    "    # Training phase\n",
    "    trainer.model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} - Training'):\n",
    "        # Move all batch tensors to device\n",
    "        input_ids = batch['input_ids'].to(trainer.device)\n",
    "        attention_mask = batch['attention_mask'].to(trainer.device)\n",
    "        labels = batch['labels'].to(trainer.device)\n",
    "\n",
    "        trainer.optimizer.zero_grad()\n",
    "        outputs = trainer.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = trainer.criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        trainer.optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_metrics = trainer.evaluate(val_loader)\n",
    "    avg_train_loss = train_loss/len(train_loader)\n",
    "\n",
    "    # Track metrics\n",
    "    trainer.tracker.update({\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        **val_metrics\n",
    "    })\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "    print('Validation Metrics:')\n",
    "    print(f'  Exact Match Accuracy: {val_metrics[\"exact_match_accuracy\"]:.4f}')\n",
    "    print(f'  Hamming Accuracy: {val_metrics[\"hamming_accuracy\"]:.4f}')\n",
    "    print(f'  F1: {val_metrics[\"f1\"]:.4f}')\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(trainer.model.state_dict(), os.path.join(trainer.output_dir, 'best_model.pt'))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "training_end_time = time.time()\n",
    "\n",
    "print(f\"Training time: {training_end_time - training_start_time:.2f} seconds\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(trainer.tracker, 1, trainer.output_dir)\n",
    "\n",
    "# Final evaluation on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "evaluation_start_time = time.time()\n",
    "\n",
    "\n",
    "trainer.model.load_state_dict(torch.load(os.path.join(trainer.output_dir, 'best_model.pt')))\n",
    "test_loss, test_predictions, test_labels, overall_metrics, per_category_metrics = trainer.evaluate_final(\n",
    "    test_loader, processor.labels\n",
    ")\n",
    "\n",
    "evaluation_end_time = time.time()\n",
    "\n",
    "\n",
    "# Create visualizations\n",
    "plot_confusion_matrices(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "plot_roc_curves(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "plot_metrics_heatmap(per_category_metrics, processor.labels, output_dir)\n",
    "plot_label_distribution(train_labels, test_predictions, processor.labels, output_dir)\n",
    "\n",
    "# Save detailed performance analysis\n",
    "create_performance_tables(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "\n",
    "# Save summary metrics\n",
    "with open(os.path.join(output_dir, 'metrics_summary.txt'), 'w') as f:\n",
    "    f.write(\"Overall Metrics:\\n\")\n",
    "    f.write(\"---------------\\n\")\n",
    "    for metric_type, metrics in overall_metrics.items():\n",
    "        f.write(f\"\\n{metric_type}:\\n\")\n",
    "        if isinstance(metrics, dict):\n",
    "            for name, value in metrics.items():\n",
    "                f.write(f\"{name}: {value:.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{metrics:.4f}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe91732-4898-406b-9a28-565aa7425999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Training time = {(training_end_time-training_start_time)/1000}s\")\n",
    "print(f\"Training time = {(evaluation_end_time-evaluation_start_time)/1000}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c418d3-8b71-4469-a290-4a9e7884ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c50d8-a66d-4d24-826a-e5165f99e175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb60ae5-8e11-4277-aeba-4593624d9284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d674da-f127-4ece-898c-de0b3a4f11c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cc185-d27d-4a94-97ed-e8f1a1d534d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a8f35-613e-4757-9f51-08bc6cab6852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed1fa1-18ee-47b9-9303-6daef966c860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e43948-81bb-4546-81d9-ef23347c90c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
