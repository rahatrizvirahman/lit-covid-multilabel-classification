{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1e57d82-c4bd-4411-b6ed-05a093b73d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 -m venv covid-venv\n",
    "!source covid-venv/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38608fde-b081-4baa-ba15-b246ef020743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "output_dir = 'output/bert'\n",
    "\n",
    "def ensure_output_dir(output_dir):\n",
    "    \"\"\"Create output directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "def plot_roc_curves(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Plot ROC curves for each class\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(\n",
    "            fpr, \n",
    "            tpr, \n",
    "            label=f'{label} (AUC = {roc_auc:.2f})'\n",
    "        )\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'roc_curves.png'))\n",
    "    plt.close()\n",
    "    \n",
    "class MetricTracker:\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "    \n",
    "    def update(self, metrics_dict):\n",
    "        for key, value in metrics_dict.items():\n",
    "            self.metrics[key].append(value)\n",
    "    \n",
    "    def get_metric(self, metric_name):\n",
    "        return self.metrics[metric_name]\n",
    "\n",
    "def plot_training_history(tracker, fold, output_dir):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(tracker.get_metric('train_loss'), label='Train Loss')\n",
    "    plt.plot(tracker.get_metric('val_loss'), label='Validation Loss')\n",
    "    plt.title(f'Loss History - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(tracker.get_metric('exact_match_accuracy'), label='Exact Match')\n",
    "    plt.plot(tracker.get_metric('hamming_accuracy'), label='Hamming')\n",
    "    plt.title(f'Accuracy History - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot F1, Precision, Recall\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(tracker.get_metric('f1'), label='F1')\n",
    "    plt.plot(tracker.get_metric('precision'), label='Precision')\n",
    "    plt.plot(tracker.get_metric('recall'), label='Recall')\n",
    "    plt.title(f'Metrics History - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'training_history_fold_{fold}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "def plot_confusion_matrices(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Plot confusion matrix for each class\"\"\"\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    n_classes = len(labels)\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, label in enumerate(labels):\n",
    "        cm = confusion_matrix(y_true[:, idx], y_pred_binary[:, idx])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Confusion Matrix - {label}')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('True')\n",
    "    \n",
    "    if len(labels) < len(axes):\n",
    "        for idx in range(len(labels), len(axes)):\n",
    "            fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrices.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_label_distribution(train_labels, test_labels, labels, output_dir):\n",
    "    \"\"\"Plot label distribution in train and test sets\"\"\"\n",
    "    train_dist = train_labels.sum(axis=0)\n",
    "    test_dist = test_labels.sum(axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_dist, width, label='Train')\n",
    "    plt.bar(x + width/2, test_dist, width, label='Test')\n",
    "    \n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Label Distribution in Train and Test Sets')\n",
    "    plt.xticks(x, labels, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'label_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "def create_performance_tables(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Create and save detailed performance tables\"\"\"\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': [],\n",
    "        'Support': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true[:, i], y_pred_binary[:, i], average='binary'\n",
    "        )\n",
    "        metrics_dict['Precision'].append(precision)\n",
    "        metrics_dict['Recall'].append(recall)\n",
    "        metrics_dict['F1-Score'].append(f1)\n",
    "        metrics_dict['Support'].append(support)\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics_dict, index=labels)\n",
    "    df_metrics.to_csv(os.path.join(output_dir, 'class_performance_metrics.csv'))\n",
    "    \n",
    "    corr_matrix = np.corrcoef(y_pred_binary.T)\n",
    "    df_corr = pd.DataFrame(corr_matrix, index=labels, columns=labels)\n",
    "    df_corr.to_csv(os.path.join(output_dir, 'prediction_correlations.csv'))\n",
    "    \n",
    "    return df_metrics, df_corr\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize labels as None - will be set later\n",
    "        self.labels = None\n",
    "        \n",
    "    def get_unique_labels(self, train_df):\n",
    "        \"\"\"Extract unique labels from training data\"\"\"\n",
    "        # Get all unique labels by splitting the semicolon-separated values\n",
    "        all_labels = set()\n",
    "        for labels in train_df['label']:\n",
    "            all_labels.update(labels.split(';'))\n",
    "        # Sort for consistency\n",
    "        self.labels = sorted(list(all_labels))\n",
    "        print(f\"Found {len(self.labels)} unique labels: {self.labels}\")\n",
    "        return self.labels\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def process_labels(self, label_text):\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Labels have not been initialized. Call get_unique_labels first.\")\n",
    "            \n",
    "        label_list = label_text.split(';')\n",
    "        label_array = np.zeros(len(self.labels))\n",
    "        for label in label_list:\n",
    "            if label in self.labels:\n",
    "                label_array[self.labels.index(label)] = 1\n",
    "            else:\n",
    "                print(f\"Warning: Unknown label encountered: {label}\")\n",
    "        return label_array\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Tokenize text using BERT tokenizer\"\"\"\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoding\n",
    "    \n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=7):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Define dimensions\n",
    "        self.bert_hidden_size = self.bert.config.hidden_size  # 768\n",
    "        self.hidden_size1 = 512\n",
    "        self.hidden_size2 = 256\n",
    "        \n",
    "        # Layers\n",
    "        self.layer1 = nn.Linear(self.bert_hidden_size, self.hidden_size1)\n",
    "        self.layer2 = nn.Linear(self.hidden_size1, self.hidden_size2)\n",
    "        self.classifier = nn.Linear(self.hidden_size2, num_labels)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()  # Added this line - initialize ReLU\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Additional layers with activation and dropout\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "def plot_metrics_heatmap(metrics_dict, labels, output_dir):\n",
    "    \"\"\"Create a heatmap of metrics for each category\"\"\"\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Precision': metrics_dict['Precision'],\n",
    "        'Recall': metrics_dict['Recall'],\n",
    "        'F1-Score': metrics_dict['F1-Score']\n",
    "    }, index=labels)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(metrics_df, annot=True, cmap='YlOrRd', fmt='.3f')\n",
    "    plt.title('Performance Metrics by Category')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'metrics_heatmap.png'))\n",
    "    plt.close()\n",
    "    \n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate various metrics for multi-label classification\"\"\"\n",
    "    # Convert predictions to binary\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Exact match accuracy (all labels must match)\n",
    "    exact_match_accuracy = np.mean(np.all(y_pred_binary == y_true, axis=1))\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_accuracy = np.mean(y_pred_binary == y_true, axis=0)\n",
    "    \n",
    "    # Hamming accuracy (proportion of correct predictions)\n",
    "    hamming_accuracy = np.mean(y_pred_binary == y_true)\n",
    "    \n",
    "    # Calculate precision, recall, f1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='samples'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'exact_match_accuracy': exact_match_accuracy,\n",
    "        'hamming_accuracy': hamming_accuracy,\n",
    "        'per_class_accuracy': per_class_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def calculate_overall_metrics(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Calculate both overall and per-category metrics\"\"\"\n",
    "    # Verify input dimensions\n",
    "    assert y_true.shape == y_pred.shape, f\"Shape mismatch: y_true {y_true.shape} != y_pred {y_pred.shape}\"\n",
    "    assert y_true.shape[1] == len(labels), f\"Number of labels mismatch: {y_true.shape[1]} != {len(labels)}\"\n",
    "    \n",
    "    # Convert predictions to binary\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Per-category metrics\n",
    "    per_category_metrics = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': [],\n",
    "        'Support': []\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPer-category Metrics:\")\n",
    "    print(\"--------------------\")\n",
    "    for i, label in enumerate(labels):\n",
    "        try:\n",
    "            precision, recall, f1, support = precision_recall_fscore_support(\n",
    "                y_true[:, i], y_pred_binary[:, i], average='binary'\n",
    "            )\n",
    "            per_category_metrics['Precision'].append(precision)\n",
    "            per_category_metrics['Recall'].append(recall)\n",
    "            per_category_metrics['F1-Score'].append(f1)\n",
    "            per_category_metrics['Support'].append(support)\n",
    "            \n",
    "            print(f\"\\n{label}:\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"F1-Score: {f1:.4f}\")\n",
    "            print(f\"Support: {support}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing label {label} at index {i}: {str(e)}\")\n",
    "            print(f\"Label shape: {y_true[:, i].shape}\")\n",
    "            print(f\"Prediction shape: {y_pred_binary[:, i].shape}\")\n",
    "            raise\n",
    "    \n",
    "    # Overall metrics\n",
    "    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='micro'\n",
    "    )\n",
    "    \n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='macro'\n",
    "    )\n",
    "    \n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='weighted'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Exact match ratio (perfect predictions across all categories)\n",
    "    exact_match = np.mean(np.all(y_pred_binary == y_true, axis=1))\n",
    "    \n",
    "    # Hamming accuracy (percentage of correct labels)\n",
    "    hamming_accuracy = np.mean(y_pred_binary == y_true)\n",
    "    \n",
    "    # Create summary dictionary\n",
    "    overall_metrics = {\n",
    "        'Micro-average': {\n",
    "            'Precision': micro_precision,\n",
    "            'Recall': micro_recall,\n",
    "            'F1-Score': micro_f1\n",
    "        },\n",
    "        'Macro-average': {\n",
    "            'Precision': macro_precision,\n",
    "            'Recall': macro_recall,\n",
    "            'F1-Score': macro_f1\n",
    "        },\n",
    "        'Weighted-average': {\n",
    "            'Precision': weighted_precision,\n",
    "            'Recall': weighted_recall,\n",
    "            'F1-Score': weighted_f1\n",
    "        },\n",
    "        'Exact Match Ratio': exact_match,\n",
    "        'Hamming Accuracy': hamming_accuracy\n",
    "    }\n",
    "    \n",
    "    # Create and display summary DataFrame\n",
    "    df_overall = pd.DataFrame({\n",
    "        'Metric': ['Precision', 'Recall', 'F1-Score'],\n",
    "        'Micro-avg': [micro_precision, micro_recall, micro_f1],\n",
    "        'Macro-avg': [macro_precision, macro_recall, macro_f1],\n",
    "        'Weighted-avg': [weighted_precision, weighted_recall, weighted_f1]\n",
    "    }).set_index('Metric')\n",
    "    \n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(\"--------------\")\n",
    "    print(f\"\\nExact Match Ratio: {exact_match:.4f}\")\n",
    "    print(f\"Hamming Accuracy: {hamming_accuracy:.4f}\")\n",
    "    print(\"\\nAveraged Metrics:\")\n",
    "    print(df_overall)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    df_overall.to_csv(os.path.join(output_dir,'overall_metrics.csv'))\n",
    "    df_categories = pd.DataFrame(per_category_metrics, index=labels)\n",
    "    df_categories.to_csv(os.path.join(output_dir,'per_category_metrics.csv'))\n",
    "    \n",
    "    return overall_metrics, per_category_metrics\n",
    "\n",
    "class BERTTrainer:\n",
    "    def __init__(self, model, device, output_dir):\n",
    "        self.model = model.to(device)  # Move model to device immediately\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "        self.tracker = MetricTracker()\n",
    "        \n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        \"\"\"Evaluate model during training\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                # Move batch tensors to device\n",
    "                input_ids = batch['input_ids'].to(trainer.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                predictions = outputs.cpu().numpy()\n",
    "                all_predictions.extend(predictions)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_labels = np.array(all_labels)\n",
    "        metrics = calculate_metrics(all_labels, all_predictions)\n",
    "        \n",
    "        return val_loss/len(loader), metrics\n",
    "    \n",
    "    def evaluate_final(self, loader, labels):\n",
    "        \"\"\"Final evaluation on test set\"\"\"\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc='Testing'):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                batch_labels = batch['labels'].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.criterion(outputs, batch_labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                # Move predictions and labels to CPU and convert to numpy\n",
    "                predictions = outputs.cpu().numpy()\n",
    "                labels_np = batch_labels.cpu().numpy()\n",
    "\n",
    "                all_predictions.append(predictions)\n",
    "                all_labels.append(labels_np)\n",
    "\n",
    "        # Concatenate all batches\n",
    "        all_predictions = np.vstack(all_predictions)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "\n",
    "        # Ensure shapes match\n",
    "        assert all_predictions.shape == all_labels.shape, \\\n",
    "            f\"Shape mismatch: predictions {all_predictions.shape} != labels {all_labels.shape}\"\n",
    "\n",
    "        overall_metrics, per_category_metrics = calculate_overall_metrics(\n",
    "            all_labels, all_predictions, labels, self.output_dir\n",
    "        )\n",
    "\n",
    "        return test_loss/len(loader), all_predictions, all_labels, overall_metrics, per_category_metrics\n",
    "\n",
    "\n",
    "def process_data(df, processor):\n",
    "    \"\"\"\n",
    "    Process dataframe into BERT-ready dataset\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame containing 'abstract' and 'label' columns\n",
    "        processor: DocumentProcessor instance\n",
    "    \n",
    "    Returns:\n",
    "        COVIDDataset instance\n",
    "    \"\"\"\n",
    "    # Clean abstracts\n",
    "    abstracts = df['abstract'].apply(processor.clean_text).values\n",
    "    \n",
    "    # Convert labels to multi-hot encoding\n",
    "    labels = np.array([processor.process_labels(label) for label in df['label']])\n",
    "    \n",
    "    # Tokenize texts\n",
    "    encodings = []\n",
    "    for abstract in tqdm(abstracts, desc=\"Tokenizing texts\"):\n",
    "        encoding = processor.tokenize_text(abstract)\n",
    "        encodings.append({\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        })\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = COVIDDataset(\n",
    "        texts=abstracts,\n",
    "        labels=labels,\n",
    "        processor=processor\n",
    "    )\n",
    "    \n",
    "    print(f\"Processed {len(dataset)} samples\")\n",
    "    return dataset\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, texts, labels, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: list of abstract texts\n",
    "            labels: numpy array of multi-hot encoded labels\n",
    "            processor: DocumentProcessor instance\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.processor.tokenize_text(text)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c2392-fed7-4acf-95f4-e1b7814f6301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1f68c10-82a6-4767-af5b-5d62b3c50732",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Found 7 unique labels: ['Case Report', 'Diagnosis', 'Epidemic Forecasting', 'Mechanism', 'Prevention', 'Transmission', 'Treatment']\n",
      "Processing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts: 100%|██████████| 24960/24960 [00:46<00:00, 540.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 24960 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts: 100%|██████████| 6239/6239 [00:11<00:00, 535.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6239 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts: 100%|██████████| 2500/2500 [00:04<00:00, 502.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|██████████| 1560/1560 [04:04<00:00,  6.37it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.2148\n",
      "Validation Loss: 0.1268\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7650\n",
      "  Hamming Accuracy: 0.9529\n",
      "  F1: 0.8873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|██████████| 1560/1560 [04:10<00:00,  6.22it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Training Loss: 0.1164\n",
      "Validation Loss: 0.1142\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7825\n",
      "  Hamming Accuracy: 0.9574\n",
      "  F1: 0.8999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|██████████| 1560/1560 [04:04<00:00,  6.38it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Training Loss: 0.0924\n",
      "Validation Loss: 0.1169\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7716\n",
      "  Hamming Accuracy: 0.9559\n",
      "  F1: 0.8957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training: 100%|██████████| 1560/1560 [04:04<00:00,  6.38it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Training Loss: 0.0726\n",
      "Validation Loss: 0.1263\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7772\n",
      "  Hamming Accuracy: 0.9564\n",
      "  F1: 0.8954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training: 100%|██████████| 1560/1560 [04:04<00:00,  6.37it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Training Loss: 0.0577\n",
      "Validation Loss: 0.1455\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7610\n",
      "  Hamming Accuracy: 0.9539\n",
      "  F1: 0.8915\n",
      "Early stopping triggered at epoch 5\n",
      "Training time: 1372.04 seconds\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_169070/3477682577.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trainer.model.load_state_dict(torch.load(os.path.join(trainer.output_dir, 'best_model.pt')))\n",
      "Testing: 100%|██████████| 157/157 [00:11<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-category Metrics:\n",
      "--------------------\n",
      "\n",
      "Case Report:\n",
      "Precision: 0.9362\n",
      "Recall: 0.8934\n",
      "F1-Score: 0.9143\n",
      "Support: None\n",
      "\n",
      "Diagnosis:\n",
      "Precision: 0.8969\n",
      "Recall: 0.8075\n",
      "F1-Score: 0.8499\n",
      "Support: None\n",
      "\n",
      "Epidemic Forecasting:\n",
      "Precision: 0.6939\n",
      "Recall: 0.8293\n",
      "F1-Score: 0.7556\n",
      "Support: None\n",
      "\n",
      "Mechanism:\n",
      "Precision: 0.9402\n",
      "Recall: 0.8589\n",
      "F1-Score: 0.8977\n",
      "Support: None\n",
      "\n",
      "Prevention:\n",
      "Precision: 0.8962\n",
      "Recall: 0.9417\n",
      "F1-Score: 0.9184\n",
      "Support: None\n",
      "\n",
      "Transmission:\n",
      "Precision: 0.7564\n",
      "Recall: 0.4609\n",
      "F1-Score: 0.5728\n",
      "Support: None\n",
      "\n",
      "Treatment:\n",
      "Precision: 0.8999\n",
      "Recall: 0.9121\n",
      "F1-Score: 0.9060\n",
      "Support: None\n",
      "\n",
      "Overall Metrics:\n",
      "--------------\n",
      "\n",
      "Exact Match Ratio: 0.7592\n",
      "Hamming Accuracy: 0.9537\n",
      "\n",
      "Averaged Metrics:\n",
      "           Micro-avg  Macro-avg  Weighted-avg\n",
      "Metric                                       \n",
      "Precision   0.900143   0.859948      0.899232\n",
      "Recall      0.872511   0.814822      0.872511\n",
      "F1-Score    0.886112   0.830648      0.883594\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10, 2500]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 123\u001b[0m\n\u001b[1;32m    113\u001b[0m evaluation_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Create visualizations\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# plot_confusion_matrices(test_labels_array, test_predictions, processor.labels, output_dir)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# plot_roc_curves(test_labels_array, test_predictions, processor.labels, output_dir)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Save detailed performance analysis\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m create_performance_tables(test_labels_array, test_predictions, processor\u001b[38;5;241m.\u001b[39mlabels, output_dir)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Save summary metrics\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics_summary.txt\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[30], line 159\u001b[0m, in \u001b[0;36mcreate_performance_tables\u001b[0;34m(y_true, y_pred, labels, output_dir)\u001b[0m\n\u001b[1;32m    151\u001b[0m metrics_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1-Score\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupport\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[1;32m    156\u001b[0m }\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labels)):\n\u001b[0;32m--> 159\u001b[0m     precision, recall, f1, support \u001b[38;5;241m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m    160\u001b[0m         y_true[:, i], y_pred_binary[:, i], average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     metrics_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(precision)\n\u001b[1;32m    163\u001b[0m     metrics_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(recall)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1721\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m \n\u001b[1;32m   1565\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m zero_division_value \u001b[38;5;241m=\u001b[39m _check_zero_division(zero_division)\n\u001b[0;32m-> 1721\u001b[0m labels \u001b[38;5;241m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1499\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1499\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    412\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10, 2500]"
     ]
    }
   ],
   "source": [
    "output_dir = 'output/bert'\n",
    "ensure_output_dir(output_dir)\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv('./dataset/BC7-LitCovid-Train.csv')\n",
    "val_df = pd.read_csv('./dataset/BC7-LitCovid-Dev.csv')\n",
    "test_df = pd.read_csv('./dataset/BC7-LitCovid-Test-GS.csv')\n",
    "\n",
    "train_labels = np.array([processor.process_labels(label) for label in train_df['label']])\n",
    "\n",
    "# Initialize processor\n",
    "processor = DocumentProcessor(model_name=model_name)\n",
    "\n",
    "# Initialize labels using training data\n",
    "processor.get_unique_labels(train_df)\n",
    "\n",
    "# Process data\n",
    "print(\"Processing datasets...\")\n",
    "train_dataset = process_data(train_df, processor)\n",
    "val_dataset = process_data(val_df, processor)\n",
    "test_dataset = process_data(test_df, processor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Setup device and model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BERTClassifier(model_name=model_name, num_labels=len(processor.labels))\n",
    "\n",
    "# Initialize trainer and run training\n",
    "trainer = BERTTrainer(model, device, output_dir)\n",
    "# test_loss, test_predictions, test_labels_array, overall_metrics, per_category_metrics = trainer.train_and_evaluate(train_loader, val_loader, test_loader, processor)\n",
    "total_epochs=5\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(total_epochs):\n",
    "    # Training phase\n",
    "    trainer.model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} - Training'):\n",
    "        # Move all batch tensors to device\n",
    "        input_ids = batch['input_ids'].to(trainer.device)\n",
    "        attention_mask = batch['attention_mask'].to(trainer.device)\n",
    "        labels = batch['labels'].to(trainer.device)\n",
    "\n",
    "        trainer.optimizer.zero_grad()\n",
    "        outputs = trainer.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = trainer.criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        trainer.optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_metrics = trainer.evaluate(val_loader)\n",
    "    avg_train_loss = train_loss/len(train_loader)\n",
    "\n",
    "    # Track metrics\n",
    "    trainer.tracker.update({\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        **val_metrics\n",
    "    })\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "    print('Validation Metrics:')\n",
    "    print(f'  Exact Match Accuracy: {val_metrics[\"exact_match_accuracy\"]:.4f}')\n",
    "    print(f'  Hamming Accuracy: {val_metrics[\"hamming_accuracy\"]:.4f}')\n",
    "    print(f'  F1: {val_metrics[\"f1\"]:.4f}')\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(trainer.model.state_dict(), os.path.join(trainer.output_dir, 'best_model.pt'))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "training_end_time = time.time()\n",
    "\n",
    "print(f\"Training time: {training_end_time - training_start_time:.2f} seconds\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(trainer.tracker, 1, trainer.output_dir)\n",
    "\n",
    "# Final evaluation on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "evaluation_start_time = time.time()\n",
    "\n",
    "trainer.model.load_state_dict(torch.load(os.path.join(trainer.output_dir, 'best_model.pt')))\n",
    "test_loss, test_predictions, test_labels, overall_metrics, per_category_metrics = trainer.evaluate_final(\n",
    "    test_loader, processor.labels\n",
    ")\n",
    "\n",
    "evaluation_end_time = time.time()\n",
    "\n",
    "\n",
    "# Create visualizations\n",
    "# plot_confusion_matrices(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "# plot_roc_curves(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "# plot_metrics_heatmap(per_category_metrics, processor.labels, output_dir)\n",
    "# plot_label_distribution(train_labels, test_predictions, processor.labels, output_dir)\n",
    "\n",
    "# Save detailed performance analysis\n",
    "create_performance_tables(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "\n",
    "# Save summary metrics\n",
    "with open(os.path.join(output_dir, 'metrics_summary.txt'), 'w') as f:\n",
    "    f.write(\"Overall Metrics:\\n\")\n",
    "    f.write(\"---------------\\n\")\n",
    "    for metric_type, metrics in overall_metrics.items():\n",
    "        f.write(f\"\\n{metric_type}:\\n\")\n",
    "        if isinstance(metrics, dict):\n",
    "            for name, value in metrics.items():\n",
    "                f.write(f\"{name}: {value:.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{metrics:.4f}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c418d3-8b71-4469-a290-4a9e7884ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "335c50d8-a66d-4d24-826a-e5165f99e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time = 1372.038405418396s\n",
      "Evaluation time = 12.563510417938232s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training time = {(training_end_time-training_start_time)}s\")\n",
    "print(f\"Evaluation time = {(evaluation_end_time-evaluation_start_time)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb60ae5-8e11-4277-aeba-4593624d9284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d674da-f127-4ece-898c-de0b3a4f11c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cc185-d27d-4a94-97ed-e8f1a1d534d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a8f35-613e-4757-9f51-08bc6cab6852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed1fa1-18ee-47b9-9303-6daef966c860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e43948-81bb-4546-81d9-ef23347c90c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
