{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38608fde-b081-4baa-ba15-b246ef020743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "output_dir = 'output/sbert'\n",
    "\n",
    "def ensure_output_dir(output_dir):\n",
    "    \"\"\"Create output directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "def plot_roc_curves(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Plot ROC curves for each class\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(\n",
    "            fpr, \n",
    "            tpr, \n",
    "            label=f'{label} (AUC = {roc_auc:.2f})'\n",
    "        )\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'roc_curves.png'))\n",
    "    plt.close()\n",
    "    \n",
    "class MetricTracker:\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "    \n",
    "    def update(self, metrics_dict):\n",
    "        for key, value in metrics_dict.items():\n",
    "            self.metrics[key].append(value)\n",
    "    \n",
    "    def get_metric(self, metric_name):\n",
    "        return self.metrics[metric_name]\n",
    "\n",
    "def plot_training_history(tracker, fold, output_dir):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(tracker.get_metric('train_loss'), label='Train Loss')\n",
    "    plt.plot(tracker.get_metric('val_loss'), label='Validation Loss')\n",
    "    plt.title(f'Loss History - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(tracker.get_metric('exact_match_accuracy'), label='Exact Match')\n",
    "    plt.plot(tracker.get_metric('hamming_accuracy'), label='Hamming')\n",
    "    plt.title(f'Accuracy History - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot F1, Precision, Recall\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(tracker.get_metric('f1'), label='F1')\n",
    "    plt.plot(tracker.get_metric('precision'), label='Precision')\n",
    "    plt.plot(tracker.get_metric('recall'), label='Recall')\n",
    "    plt.title(f'Metrics History - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'training_history_fold_{fold}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "def plot_confusion_matrices(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Plot confusion matrix for each class\"\"\"\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    n_classes = len(labels)\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, label in enumerate(labels):\n",
    "        cm = confusion_matrix(y_true[:, idx], y_pred_binary[:, idx])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Confusion Matrix - {label}')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('True')\n",
    "    \n",
    "    if len(labels) < len(axes):\n",
    "        for idx in range(len(labels), len(axes)):\n",
    "            fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrices.png'))\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def plot_label_distribution(train_labels, test_labels, labels, output_dir):\n",
    "    \"\"\"Plot label distribution in train and test sets\"\"\"\n",
    "    train_dist = train_labels.sum(axis=0)\n",
    "    test_dist = test_labels.sum(axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_dist, width, label='Train')\n",
    "    plt.bar(x + width/2, test_dist, width, label='Test')\n",
    "    \n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Label Distribution in Train and Test Sets')\n",
    "    plt.xticks(x, labels, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'label_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "def create_performance_tables(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Create and save detailed performance tables\"\"\"\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': [],\n",
    "        'Support': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true[:, i], y_pred_binary[:, i], average='binary'\n",
    "        )\n",
    "        metrics_dict['Precision'].append(precision)\n",
    "        metrics_dict['Recall'].append(recall)\n",
    "        metrics_dict['F1-Score'].append(f1)\n",
    "        metrics_dict['Support'].append(support)\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics_dict, index=labels)\n",
    "    df_metrics.to_csv(os.path.join(output_dir, 'class_performance_metrics.csv'))\n",
    "    \n",
    "    corr_matrix = np.corrcoef(y_pred_binary.T)\n",
    "    df_corr = pd.DataFrame(corr_matrix, index=labels, columns=labels)\n",
    "    df_corr.to_csv(os.path.join(output_dir, 'prediction_correlations.csv'))\n",
    "    \n",
    "    return df_metrics, df_corr\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.labels = ['Treatment', 'Prevention', 'Diagnosis', 'Mechanism', \n",
    "                      'Transmission', 'Epidemic Forecasting', 'Case Report']\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def process_labels(self, label_text):\n",
    "        label_list = label_text.split(';')\n",
    "        label_array = np.zeros(len(self.labels))\n",
    "        for label in label_list:\n",
    "            if label in self.labels:\n",
    "                label_array[self.labels.index(label)] = 1\n",
    "        return label_array\n",
    "    \n",
    "    def generate_embeddings(self, texts, batch_size=32, cache_file=None):\n",
    "        if cache_file and os.path.exists(cache_file):\n",
    "            print(f\"Loading cached embeddings from {cache_file}\")\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        \n",
    "        print(\"Generating new embeddings...\")\n",
    "        embeddings = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch_texts)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        embeddings = np.array(embeddings)\n",
    "        \n",
    "        if cache_file:\n",
    "            print(f\"Caching embeddings to {cache_file}\")\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(embeddings, f)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        assert len(embeddings) == len(labels), \"Embeddings and labels must have same length\"\n",
    "        self.embeddings = torch.FloatTensor(embeddings)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        print(f\"Dataset size: {len(self.embeddings)} samples\") \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.embeddings):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset of size {len(self.embeddings)}\")\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "class TopicClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=512, hidden_dim2=256, num_classes=7):\n",
    "        super(TopicClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.layer2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.layer3 = nn.Linear(hidden_dim2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "# Modify train_fold function to use MetricTracker\n",
    "def train_fold(model, train_loader, val_loader, criterion, optimizer, device, fold, output_dir):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    \n",
    "    tracker = MetricTracker()\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_embeddings, batch_labels in tqdm(train_loader, desc=f'Fold {fold}, Epoch {epoch+1} - Training'):\n",
    "            batch_embeddings = batch_embeddings.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_embeddings)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_metrics = evaluate_fold(model, val_loader, criterion, device, fold)\n",
    "        \n",
    "        avg_train_loss = train_loss/len(train_loader)\n",
    "        \n",
    "        # Track metrics\n",
    "        tracker.update({\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            **val_metrics\n",
    "        })\n",
    "        \n",
    "        print(f'Fold {fold}, Epoch {epoch+1}')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        print('Validation Metrics:')\n",
    "        print(f'  Exact Match Accuracy: {val_metrics[\"exact_match_accuracy\"]:.4f}')\n",
    "        print(f'  Hamming Accuracy: {val_metrics[\"hamming_accuracy\"]:.4f}')\n",
    "        print(f'  F1: {val_metrics[\"f1\"]:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, f'best_model_fold_{fold}.pt'))\n",
    "\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Plot training history for this fold\n",
    "    plot_training_history(tracker, fold, output_dir)\n",
    "    return best_val_loss, val_metrics\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, labels, output_dir):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_embeddings, batch_labels in tqdm(test_loader, desc='Testing'):\n",
    "            batch_embeddings = batch_embeddings.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_embeddings)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            predictions = outputs.cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    overall_metrics, per_category_metrics = calculate_overall_metrics(\n",
    "        all_labels, all_predictions, labels, output_dir\n",
    "    )\n",
    "    \n",
    "    return test_loss/len(test_loader), all_predictions, all_labels, overall_metrics, per_category_metrics\n",
    "\n",
    "def plot_metrics_heatmap(metrics_dict, labels, output_dir):\n",
    "    \"\"\"Create a heatmap of metrics for each category\"\"\"\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Precision': metrics_dict['Precision'],\n",
    "        'Recall': metrics_dict['Recall'],\n",
    "        'F1-Score': metrics_dict['F1-Score']\n",
    "    }, index=labels)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(metrics_df, annot=True, cmap='YlOrRd', fmt='.3f')\n",
    "    plt.title('Performance Metrics by Category')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'metrics_heatmap.png'))\n",
    "    plt.close()\n",
    "    \n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate various metrics for multi-label classification\"\"\"\n",
    "    # Convert predictions to binary\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Exact match accuracy (all labels must match)\n",
    "    exact_match_accuracy = np.mean(np.all(y_pred_binary == y_true, axis=1))\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_accuracy = np.mean(y_pred_binary == y_true, axis=0)\n",
    "    \n",
    "    # Hamming accuracy (proportion of correct predictions)\n",
    "    hamming_accuracy = np.mean(y_pred_binary == y_true)\n",
    "    \n",
    "    # Calculate precision, recall, f1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='samples'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'exact_match_accuracy': exact_match_accuracy,\n",
    "        'hamming_accuracy': hamming_accuracy,\n",
    "        'per_class_accuracy': per_class_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def calculate_overall_metrics(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"\n",
    "    Calculate both overall and per-category metrics\n",
    "    \"\"\"\n",
    "    # Convert predictions to binary\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Per-category metrics\n",
    "    per_category_metrics = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': [],\n",
    "        'Support': []\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPer-category Metrics:\")\n",
    "    print(\"--------------------\")\n",
    "    for i, label in enumerate(labels):\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true[:, i], y_pred_binary[:, i], average='binary'\n",
    "        )\n",
    "        per_category_metrics['Precision'].append(precision)\n",
    "        per_category_metrics['Recall'].append(recall)\n",
    "        per_category_metrics['F1-Score'].append(f1)\n",
    "        per_category_metrics['Support'].append(support)\n",
    "        \n",
    "        print(f\"\\n{label}:\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        print(f\"Support: {support}\")\n",
    "    \n",
    "    # Overall metrics (micro average)\n",
    "    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='micro'\n",
    "    )\n",
    "    \n",
    "    # Overall metrics (macro average)\n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='macro'\n",
    "    )\n",
    "    \n",
    "    # Overall metrics (weighted average)\n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Exact match ratio (perfect predictions across all categories)\n",
    "    exact_match = np.mean(np.all(y_pred_binary == y_true, axis=1))\n",
    "    \n",
    "    # Hamming accuracy (percentage of correct labels)\n",
    "    hamming_accuracy = np.mean(y_pred_binary == y_true)\n",
    "    \n",
    "    # Create summary dictionary\n",
    "    overall_metrics = {\n",
    "        'Micro-average': {\n",
    "            'Precision': micro_precision,\n",
    "            'Recall': micro_recall,\n",
    "            'F1-Score': micro_f1\n",
    "        },\n",
    "        'Macro-average': {\n",
    "            'Precision': macro_precision,\n",
    "            'Recall': macro_recall,\n",
    "            'F1-Score': macro_f1\n",
    "        },\n",
    "        'Weighted-average': {\n",
    "            'Precision': weighted_precision,\n",
    "            'Recall': weighted_recall,\n",
    "            'F1-Score': weighted_f1\n",
    "        },\n",
    "        'Exact Match Ratio': exact_match,\n",
    "        'Hamming Accuracy': hamming_accuracy\n",
    "    }\n",
    "    \n",
    "    # Create and display summary DataFrame\n",
    "    df_overall = pd.DataFrame({\n",
    "        'Metric': ['Precision', 'Recall', 'F1-Score'],\n",
    "        'Micro-avg': [micro_precision, micro_recall, micro_f1],\n",
    "        'Macro-avg': [macro_precision, macro_recall, macro_f1],\n",
    "        'Weighted-avg': [weighted_precision, weighted_recall, weighted_f1]\n",
    "    }).set_index('Metric')\n",
    "    \n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(\"--------------\")\n",
    "    print(f\"\\nExact Match Ratio: {exact_match:.4f}\")\n",
    "    print(f\"Hamming Accuracy: {hamming_accuracy:.4f}\")\n",
    "    print(\"\\nAveraged Metrics:\")\n",
    "    print(df_overall)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    df_overall.to_csv(os.path.join(output_dir,'overall_metrics.csv'))\n",
    "    df_categories = pd.DataFrame(per_category_metrics, index=labels)\n",
    "    df_categories.to_csv(os.path.join(output_dir,'per_category_metrics.csv'))\n",
    "    \n",
    "    return overall_metrics, per_category_metrics\n",
    "\n",
    "def evaluate_fold(model, val_loader, criterion, device, fold):\n",
    "    \"\"\"Evaluate model on validation set during training\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_embeddings, batch_labels in val_loader:\n",
    "            batch_embeddings = batch_embeddings.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_embeddings)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            predictions = outputs.cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate metrics with binary predictions\n",
    "    metrics = calculate_metrics(all_labels, all_predictions)\n",
    "    \n",
    "    return val_loss/len(val_loader), metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cde2245-1aee-4f91-976b-b17a4e0359b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Loading cached embeddings from output/sbert/train_embeddings_cache.pkl\n",
      "Processing validation data...\n",
      "Loading cached embeddings from output/sbert/val_embeddings_cache.pkl\n",
      "Processing test data...\n",
      "Loading cached embeddings from output/sbert/test_embeddings_cache.pkl\n",
      "Embedding dimension: 768\n",
      "Dataset size: 24960 samples\n",
      "Dataset size: 6239 samples\n",
      "Dataset size: 2500 samples\n",
      "Using device: cuda\n",
      "\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|██████████| 780/780 [00:04<00:00, 191.40it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.1908\n",
      "Validation Loss: 0.1400\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7426\n",
      "  Hamming Accuracy: 0.9459\n",
      "  F1: 0.8618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|██████████| 780/780 [00:04<00:00, 183.59it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Training Loss: 0.1402\n",
      "Validation Loss: 0.1312\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7458\n",
      "  Hamming Accuracy: 0.9483\n",
      "  F1: 0.8661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|██████████| 780/780 [00:04<00:00, 186.02it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Training Loss: 0.1306\n",
      "Validation Loss: 0.1285\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7493\n",
      "  Hamming Accuracy: 0.9489\n",
      "  F1: 0.8706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training: 100%|██████████| 780/780 [00:04<00:00, 192.39it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Training Loss: 0.1256\n",
      "Validation Loss: 0.1278\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7559\n",
      "  Hamming Accuracy: 0.9508\n",
      "  F1: 0.8770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training: 100%|██████████| 780/780 [00:04<00:00, 182.53it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Training Loss: 0.1202\n",
      "Validation Loss: 0.1281\n",
      "Validation Metrics:\n",
      "  Exact Match Accuracy: 0.7581\n",
      "  Hamming Accuracy: 0.9501\n",
      "  F1: 0.8751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_165154/3823143701.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(output_dir, 'best_model.pt')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 79/79 [00:00<00:00, 415.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-category Metrics:\n",
      "--------------------\n",
      "\n",
      "Treatment:\n",
      "Precision: 0.8874\n",
      "Recall: 0.8908\n",
      "F1-Score: 0.8891\n",
      "Support: None\n",
      "\n",
      "Prevention:\n",
      "Precision: 0.9032\n",
      "Recall: 0.9374\n",
      "F1-Score: 0.9200\n",
      "Support: None\n",
      "\n",
      "Diagnosis:\n",
      "Precision: 0.9265\n",
      "Recall: 0.6981\n",
      "F1-Score: 0.7962\n",
      "Support: None\n",
      "\n",
      "Mechanism:\n",
      "Precision: 0.9212\n",
      "Recall: 0.8042\n",
      "F1-Score: 0.8588\n",
      "Support: None\n",
      "\n",
      "Transmission:\n",
      "Precision: 0.7470\n",
      "Recall: 0.4844\n",
      "F1-Score: 0.5877\n",
      "Support: None\n",
      "\n",
      "Epidemic Forecasting:\n",
      "Precision: 0.6032\n",
      "Recall: 0.9268\n",
      "F1-Score: 0.7308\n",
      "Support: None\n",
      "\n",
      "Case Report:\n",
      "Precision: 0.9018\n",
      "Recall: 0.7462\n",
      "F1-Score: 0.8167\n",
      "Support: None\n",
      "\n",
      "Overall Metrics:\n",
      "--------------\n",
      "\n",
      "Exact Match Ratio: 0.7244\n",
      "Hamming Accuracy: 0.9446\n",
      "\n",
      "Averaged Metrics:\n",
      "           Micro-avg  Macro-avg  Weighted-avg\n",
      "Metric                                       \n",
      "Precision   0.895161   0.841472      0.897147\n",
      "Recall      0.828816   0.783982      0.828816\n",
      "F1-Score    0.860712   0.799880      0.857292\n"
     ]
    }
   ],
   "source": [
    "model_name='all-mpnet-base-v2'\n",
    "ensure_output_dir(output_dir)\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv('./dataset/BC7-LitCovid-Train.csv')\n",
    "val_df = pd.read_csv('./dataset/BC7-LitCovid-Dev.csv')  \n",
    "test_df = pd.read_csv('./dataset/BC7-LitCovid-Test-GS.csv')\n",
    "\n",
    "# Initialize processor\n",
    "processor = DocumentProcessor(model_name)\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "train_abstracts = train_df['abstract'].apply(processor.clean_text).values\n",
    "train_labels = np.array([processor.process_labels(label) for label in train_df['label']])\n",
    "train_embeddings = processor.generate_embeddings(train_abstracts, cache_file=f'{output_dir}/train_embeddings_cache.pkl')\n",
    "\n",
    "# Process validation data\n",
    "print(\"Processing validation data...\")\n",
    "val_abstracts = val_df['abstract'].apply(processor.clean_text).values\n",
    "val_labels = np.array([processor.process_labels(label) for label in val_df['label']])\n",
    "val_embeddings = processor.generate_embeddings(val_abstracts, cache_file=f'{output_dir}/val_embeddings_cache.pkl')\n",
    "\n",
    "# Process test data\n",
    "print(\"Processing test data...\")\n",
    "test_abstracts = test_df['abstract'].apply(processor.clean_text).values\n",
    "test_labels = np.array([processor.process_labels(label) for label in test_df['label']])\n",
    "test_embeddings = processor.generate_embeddings(test_abstracts, cache_file=f'{output_dir}/test_embeddings_cache.pkl')\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dim = train_embeddings.shape[1]\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = COVIDDataset(train_embeddings, train_labels)\n",
    "val_dataset = COVIDDataset(val_embeddings, val_labels)\n",
    "test_dataset = COVIDDataset(test_embeddings, test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Initialize model, criterion, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = TopicClassifier(input_dim=embedding_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining model...\")\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "tracker = MetricTracker()\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "total_epoch=5\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_embeddings, batch_labels in tqdm(train_loader, desc=f'Epoch {epoch+1} - Training'):\n",
    "        batch_embeddings = batch_embeddings.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_embeddings)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_metrics = evaluate_fold(model, val_loader, criterion, device, 1)\n",
    "\n",
    "    avg_train_loss = train_loss/len(train_loader)\n",
    "\n",
    "    # Track metrics\n",
    "    tracker.update({\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        **val_metrics\n",
    "    })\n",
    "\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "    print('Validation Metrics:')\n",
    "    print(f'  Exact Match Accuracy: {val_metrics[\"exact_match_accuracy\"]:.4f}')\n",
    "    print(f'  Hamming Accuracy: {val_metrics[\"hamming_accuracy\"]:.4f}')\n",
    "    print(f'  F1: {val_metrics[\"f1\"]:.4f}')\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, 'best_model.pt'))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "training_end_time = time.time()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(tracker, 1, output_dir)\n",
    "\n",
    "evaluation_start_time = time.time()\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "model.load_state_dict(torch.load(os.path.join(output_dir, 'best_model.pt')))\n",
    "test_loss, test_predictions, test_labels_array, overall_metrics, per_category_metrics = evaluate_model(\n",
    "    model, test_loader, criterion, device, processor.labels, output_dir\n",
    ")\n",
    "\n",
    "evaluation_end_time = time.time()\n",
    "\n",
    "# Create visualizations\n",
    "plot_confusion_matrices(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "plot_roc_curves(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "plot_metrics_heatmap(per_category_metrics, processor.labels, output_dir)\n",
    "plot_label_distribution(train_labels, test_labels_array, processor.labels, output_dir)\n",
    "\n",
    "# Save detailed performance analysis\n",
    "create_performance_tables(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "\n",
    "# Save summary metrics\n",
    "with open(os.path.join(output_dir, 'metrics_summary.txt'), 'w') as f:\n",
    "    f.write(\"Overall Metrics:\\n\")\n",
    "    f.write(\"---------------\\n\")\n",
    "    for metric_type, metrics in overall_metrics.items():\n",
    "        f.write(f\"\\n{metric_type}:\\n\")\n",
    "        if isinstance(metrics, dict):\n",
    "            for name, value in metrics.items():\n",
    "                f.write(f\"{name}: {value:.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{metrics:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c418d3-8b71-4469-a290-4a9e7884ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b59f839-6470-4a82-8e52-b65d7dcd2667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time = 23.394458770751953s\n",
      "Evaluation time = 0.3761427402496338s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training time = {(training_end_time-training_start_time)}s\")\n",
    "print(f\"Evaluation time = {(evaluation_end_time-evaluation_start_time)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e9ad85-d0f1-4755-81e4-0a0b7bbe14b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dcdab4-3b44-4d88-ae42-190f06cf0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "Per-category Metrics:\n",
    "--------------------\n",
    "\n",
    "Treatment:\n",
    "Precision: 0.8829\n",
    "Recall: 0.8957\n",
    "F1-Score: 0.8892\n",
    "Support: None\n",
    "\n",
    "Prevention:\n",
    "Precision: 0.9214\n",
    "Recall: 0.9244\n",
    "F1-Score: 0.9229\n",
    "Support: None\n",
    "\n",
    "Diagnosis:\n",
    "Precision: 0.8723\n",
    "Recall: 0.8324\n",
    "F1-Score: 0.8519\n",
    "Support: None\n",
    "\n",
    "Mechanism:\n",
    "Precision: 0.8985\n",
    "Recall: 0.8589\n",
    "F1-Score: 0.8783\n",
    "Support: None\n",
    "\n",
    "Transmission:\n",
    "Precision: 0.7800\n",
    "Recall: 0.3047\n",
    "F1-Score: 0.4382\n",
    "Support: None\n",
    "\n",
    "Epidemic Forecasting:\n",
    "Precision: 0.7381\n",
    "Recall: 0.7561\n",
    "F1-Score: 0.7470\n",
    "Support: None\n",
    "\n",
    "Case Report:\n",
    "Precision: 0.9329\n",
    "Recall: 0.7056\n",
    "F1-Score: 0.8035\n",
    "Support: None\n",
    "\n",
    "Overall Metrics:\n",
    "--------------\n",
    "\n",
    "Exact Match Ratio: 0.7392\n",
    "Hamming Accuracy: 0.9482\n",
    "\n",
    "Averaged Metrics:\n",
    "           Micro-avg  Macro-avg  Weighted-avg\n",
    "Metric                                       \n",
    "Precision   0.892495   0.860866      0.890520\n",
    "Recall      0.851770   0.753963      0.851770\n",
    "F1-Score    0.871657   0.790132      0.866422\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
