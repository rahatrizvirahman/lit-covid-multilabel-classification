{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e57d82-c4bd-4411-b6ed-05a093b73d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 -m venv covid-venv\n",
    "!source covid-venv/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38608fde-b081-4baa-ba15-b246ef020743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "output_dir = 'output/bert'\n",
    "\n",
    "def ensure_output_dir(output_dir):\n",
    "    \"\"\"Create output directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "def plot_roc_curves(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Plot ROC curves for each class\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(\n",
    "            fpr, \n",
    "            tpr, \n",
    "            label=f'{label} (AUC = {roc_auc:.2f})'\n",
    "        )\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'roc_curves.png'))\n",
    "    plt.close()\n",
    "    \n",
    "class MetricTracker:\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "    \n",
    "    def update(self, metrics_dict):\n",
    "        for key, value in metrics_dict.items():\n",
    "            self.metrics[key].append(value)\n",
    "    \n",
    "    def get_metric(self, metric_name):\n",
    "        return self.metrics[metric_name]\n",
    "\n",
    "def plot_training_history(tracker, fold, output_dir):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(tracker.get_metric('train_loss'), label='Train Loss')\n",
    "    plt.plot(tracker.get_metric('val_loss'), label='Validation Loss')\n",
    "    plt.title(f'Loss History - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot metrics\n",
    "    plt.subplot(2, 2, 2)\n",
    "    if 'accuracy' in tracker.metrics:\n",
    "        plt.plot(tracker.get_metric('accuracy'), label='Accuracy')\n",
    "    if 'f1' in tracker.metrics:\n",
    "        plt.plot(tracker.get_metric('f1'), label='F1')\n",
    "    plt.title(f'Metrics History - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'training_history_fold_{fold}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "def plot_confusion_matrices(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Plot confusion matrix for each class\"\"\"\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    n_classes = len(labels)\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, label in enumerate(labels):\n",
    "        cm = confusion_matrix(y_true[:, idx], y_pred_binary[:, idx])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Confusion Matrix - {label}')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('True')\n",
    "    \n",
    "    if len(labels) < len(axes):\n",
    "        for idx in range(len(labels), len(axes)):\n",
    "            fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrices.png'))\n",
    "    plt.close()\n",
    "\n",
    "def plot_label_distribution(train_labels, test_labels, labels, output_dir):\n",
    "    \"\"\"Plot label distribution in train and test sets\"\"\"\n",
    "    train_dist = train_labels.sum(axis=0)\n",
    "    test_dist = test_labels.sum(axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_dist, width, label='Train')\n",
    "    plt.bar(x + width/2, test_dist, width, label='Test')\n",
    "    \n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Label Distribution in Train and Test Sets')\n",
    "    plt.xticks(x, labels, rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'label_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "def create_performance_tables(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Create and save detailed performance tables\"\"\"\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': [],\n",
    "        'Support': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true[:, i], y_pred_binary[:, i], average='binary'\n",
    "        )\n",
    "        metrics_dict['Precision'].append(precision)\n",
    "        metrics_dict['Recall'].append(recall)\n",
    "        metrics_dict['F1-Score'].append(f1)\n",
    "        metrics_dict['Support'].append(support)\n",
    "    \n",
    "    df_metrics = pd.DataFrame(metrics_dict, index=labels)\n",
    "    df_metrics.to_csv(os.path.join(output_dir, 'class_performance_metrics.csv'))\n",
    "    \n",
    "    corr_matrix = np.corrcoef(y_pred_binary.T)\n",
    "    df_corr = pd.DataFrame(corr_matrix, index=labels, columns=labels)\n",
    "    df_corr.to_csv(os.path.join(output_dir, 'prediction_correlations.csv'))\n",
    "    \n",
    "    return df_metrics, df_corr\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, embedding_dim=300, max_length=200):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_length = max_length\n",
    "        self.word2vec = None\n",
    "        self.word2idx = {}\n",
    "        self.vocab_size = 0\n",
    "        self.labels = None\n",
    "        \n",
    "    def load_word2vec(self, texts):\n",
    "        \"\"\"Train Word2Vec on our corpus\"\"\"\n",
    "        # Tokenize all texts\n",
    "        tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "        \n",
    "        # Train Word2Vec\n",
    "        self.word2vec = Word2Vec(sentences=tokenized_texts, \n",
    "                                vector_size=self.embedding_dim, \n",
    "                                window=5, \n",
    "                                min_count=1, \n",
    "                                workers=4)\n",
    "        \n",
    "        # Create word to index mapping\n",
    "        self.word2idx = {word: idx + 1 for idx, word in \n",
    "                        enumerate(self.word2vec.wv.index_to_key)}\n",
    "        self.vocab_size = len(self.word2idx) + 1  # +1 for padding\n",
    "        \n",
    "        # Create embedding matrix\n",
    "        self.embedding_matrix = np.zeros((self.vocab_size, self.embedding_dim))\n",
    "        for word, idx in self.word2idx.items():\n",
    "            self.embedding_matrix[idx] = self.word2vec.wv[word]\n",
    "    \n",
    "    def get_unique_labels(self, train_df):\n",
    "        all_labels = set()\n",
    "        for labels in train_df['label']:\n",
    "            all_labels.update(labels.split(';'))\n",
    "        self.labels = sorted(list(all_labels))\n",
    "        return self.labels\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        return str(text).lower()\n",
    "    \n",
    "    def process_labels(self, label_text):\n",
    "        label_list = label_text.split(';')\n",
    "        label_array = np.zeros(len(self.labels))\n",
    "        for label in label_list:\n",
    "            if label in self.labels:\n",
    "                label_array[self.labels.index(label)] = 1\n",
    "        return label_array\n",
    "    \n",
    "    def text_to_sequence(self, text):\n",
    "        \"\"\"Convert text to sequence of word indices\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        sequence = [self.word2idx.get(token, 0) for token in tokens[:self.max_length]]\n",
    "        # Pad sequence\n",
    "        if len(sequence) < self.max_length:\n",
    "            sequence = sequence + [0] * (self.max_length - len(sequence))\n",
    "        else:\n",
    "            sequence = sequence[:self.max_length]\n",
    "        return torch.LongTensor(sequence)\n",
    "    \n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim1=512, hidden_dim2=256, num_classes=7):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim1, \n",
    "                           num_layers=2, \n",
    "                           bidirectional=True, \n",
    "                           batch_first=True,\n",
    "                           dropout=0.2)\n",
    "        \n",
    "        # Additional layers\n",
    "        self.fc1 = nn.Linear(hidden_dim1 * 2, hidden_dim1)  # *2 for bidirectional\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, num_classes)\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_dim2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get embeddings\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)  # [batch_size, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Get final hidden state for both directions\n",
    "        lstm_out = lstm_out[:, -1, :]  # [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Additional layers with residual connections\n",
    "        x = self.dropout(lstm_out)\n",
    "        x = F.relu(self.batch_norm1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.batch_norm2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def plot_metrics_heatmap(metrics_dict, labels, output_dir):\n",
    "    \"\"\"Create a heatmap of metrics for each category\"\"\"\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Precision': metrics_dict['Precision'],\n",
    "        'Recall': metrics_dict['Recall'],\n",
    "        'F1-Score': metrics_dict['F1-Score']\n",
    "    }, index=labels)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(metrics_df, annot=True, cmap='YlOrRd', fmt='.3f')\n",
    "    plt.title('Performance Metrics by Category')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'metrics_heatmap.png'))\n",
    "    plt.close()\n",
    "    \n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate various metrics for multi-label classification\"\"\"\n",
    "    # Convert predictions to binary\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Exact match accuracy (all labels must match)\n",
    "    exact_match_accuracy = np.mean(np.all(y_pred_binary == y_true, axis=1))\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_accuracy = np.mean(y_pred_binary == y_true, axis=0)\n",
    "    \n",
    "    # Hamming accuracy (proportion of correct predictions)\n",
    "    hamming_accuracy = np.mean(y_pred_binary == y_true)\n",
    "    \n",
    "    # Calculate precision, recall, f1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='samples'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'exact_match_accuracy': exact_match_accuracy,\n",
    "        'hamming_accuracy': hamming_accuracy,\n",
    "        'per_class_accuracy': per_class_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def calculate_overall_metrics(y_true, y_pred, labels, output_dir):\n",
    "    \"\"\"Calculate both overall and per-category metrics\"\"\"\n",
    "    # Verify input dimensions\n",
    "    assert y_true.shape == y_pred.shape, f\"Shape mismatch: y_true {y_true.shape} != y_pred {y_pred.shape}\"\n",
    "    assert y_true.shape[1] == len(labels), f\"Number of labels mismatch: {y_true.shape[1]} != {len(labels)}\"\n",
    "    \n",
    "    # Convert predictions to binary\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Per-category metrics\n",
    "    per_category_metrics = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': [],\n",
    "        'Support': []\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPer-category Metrics:\")\n",
    "    print(\"--------------------\")\n",
    "    for i, label in enumerate(labels):\n",
    "        try:\n",
    "            precision, recall, f1, support = precision_recall_fscore_support(\n",
    "                y_true[:, i], y_pred_binary[:, i], average='binary'\n",
    "            )\n",
    "            per_category_metrics['Precision'].append(precision)\n",
    "            per_category_metrics['Recall'].append(recall)\n",
    "            per_category_metrics['F1-Score'].append(f1)\n",
    "            per_category_metrics['Support'].append(support)\n",
    "            \n",
    "            print(f\"\\n{label}:\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"F1-Score: {f1:.4f}\")\n",
    "            print(f\"Support: {support}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing label {label} at index {i}: {str(e)}\")\n",
    "            print(f\"Label shape: {y_true[:, i].shape}\")\n",
    "            print(f\"Prediction shape: {y_pred_binary[:, i].shape}\")\n",
    "            raise\n",
    "    \n",
    "    # Overall metrics\n",
    "    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='micro'\n",
    "    )\n",
    "    \n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='macro'\n",
    "    )\n",
    "    \n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred_binary, average='weighted'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Exact match ratio (perfect predictions across all categories)\n",
    "    exact_match = np.mean(np.all(y_pred_binary == y_true, axis=1))\n",
    "    \n",
    "    # Hamming accuracy (percentage of correct labels)\n",
    "    hamming_accuracy = np.mean(y_pred_binary == y_true)\n",
    "    \n",
    "    # Create summary dictionary\n",
    "    overall_metrics = {\n",
    "        'Micro-average': {\n",
    "            'Precision': micro_precision,\n",
    "            'Recall': micro_recall,\n",
    "            'F1-Score': micro_f1\n",
    "        },\n",
    "        'Macro-average': {\n",
    "            'Precision': macro_precision,\n",
    "            'Recall': macro_recall,\n",
    "            'F1-Score': macro_f1\n",
    "        },\n",
    "        'Weighted-average': {\n",
    "            'Precision': weighted_precision,\n",
    "            'Recall': weighted_recall,\n",
    "            'F1-Score': weighted_f1\n",
    "        },\n",
    "        'Exact Match Ratio': exact_match,\n",
    "        'Hamming Accuracy': hamming_accuracy\n",
    "    }\n",
    "    \n",
    "    # Create and display summary DataFrame\n",
    "    df_overall = pd.DataFrame({\n",
    "        'Metric': ['Precision', 'Recall', 'F1-Score'],\n",
    "        'Micro-avg': [micro_precision, micro_recall, micro_f1],\n",
    "        'Macro-avg': [macro_precision, macro_recall, macro_f1],\n",
    "        'Weighted-avg': [weighted_precision, weighted_recall, weighted_f1]\n",
    "    }).set_index('Metric')\n",
    "    \n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(\"--------------\")\n",
    "    print(f\"\\nExact Match Ratio: {exact_match:.4f}\")\n",
    "    print(f\"Hamming Accuracy: {hamming_accuracy:.4f}\")\n",
    "    print(\"\\nAveraged Metrics:\")\n",
    "    print(df_overall)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    df_overall.to_csv(os.path.join(output_dir,'overall_metrics.csv'))\n",
    "    df_categories = pd.DataFrame(per_category_metrics, index=labels)\n",
    "    df_categories.to_csv(os.path.join(output_dir,'per_category_metrics.csv'))\n",
    "    \n",
    "    return overall_metrics, per_category_metrics\n",
    "\n",
    "# def train_model(model, train_loader, val_loader, num_epochs=5, device='cuda'):\n",
    "#     model = model.to(device)\n",
    "#     criterion = nn.BCELoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "    \n",
    "#     best_val_loss = float('inf')\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} Training'):\n",
    "#             sequences = batch['sequence'].to(device)\n",
    "#             labels = batch['labels'].to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(sequences)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             train_loss += loss.item()\n",
    "        \n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(val_loader, desc='Validation'):\n",
    "#                 sequences = batch['sequence'].to(device)\n",
    "#                 labels = batch['labels'].to(device)\n",
    "                \n",
    "#                 outputs = model(sequences)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 val_loss += loss.item()\n",
    "        \n",
    "#         # Print metrics\n",
    "#         avg_train_loss = train_loss / len(train_loader)\n",
    "#         avg_val_loss = val_loss / len(val_loader)\n",
    "#         print(f'Epoch {epoch+1}:')\n",
    "#         print(f'Average Training Loss: {avg_train_loss:.4f}')\n",
    "#         print(f'Average Validation Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "#         # Learning rate scheduling\n",
    "#         scheduler.step(avg_val_loss)\n",
    "        \n",
    "#         # Save best model\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             torch.save(model.state_dict(), 'best_bilstm_model.pt')\n",
    "\n",
    "class BERTTrainer:\n",
    "    def __init__(self, model, device, output_dir):\n",
    "        self.model = model.to(device)  # Move model to device immediately\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "        self.tracker = MetricTracker()\n",
    "        \n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        \"\"\"Evaluate model during training\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                # Move batch tensors to device\n",
    "                input_ids = batch['input_ids'].to(trainer.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                predictions = outputs.cpu().numpy()\n",
    "                all_predictions.extend(predictions)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_labels = np.array(all_labels)\n",
    "        metrics = calculate_metrics(all_labels, all_predictions)\n",
    "        \n",
    "        return val_loss/len(loader), metrics\n",
    "    \n",
    "def evaluate_final(model, test_loader, labels, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Testing'):\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            batch_labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            predictions = outputs.cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    overall_metrics, per_category_metrics = calculate_overall_metrics(\n",
    "        all_labels, all_predictions, labels, output_dir\n",
    "    )\n",
    "\n",
    "    return test_loss/len(test_loader), all_predictions, all_labels, overall_metrics, per_category_metrics\n",
    "\n",
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "    \n",
    "    def update(self, metrics_dict):\n",
    "        \"\"\"Update metrics with a dictionary of values\"\"\"\n",
    "        for key, value in metrics_dict.items():\n",
    "            self.metrics[key].append(value)\n",
    "    \n",
    "    def get_metric(self, metric_name):\n",
    "        \"\"\"Get the list of values for a specific metric\"\"\"\n",
    "        return self.metrics[metric_name]\n",
    "        \n",
    "def plot_training_curves(tracker, output_dir):\n",
    "    \"\"\"Plot training and validation curves\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(tracker.train_losses, label='Train Loss')\n",
    "    plt.plot(tracker.val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(os.path.join(output_dir, 'training_curves.png'))\n",
    "    plt.close()\n",
    "\n",
    "def process_data(df, processor):\n",
    "    \"\"\"\n",
    "    Process dataframe into BERT-ready dataset\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame containing 'abstract' and 'label' columns\n",
    "        processor: DocumentProcessor instance\n",
    "    \n",
    "    Returns:\n",
    "        COVIDDataset instance\n",
    "    \"\"\"\n",
    "    # Clean abstracts\n",
    "    abstracts = df['abstract'].apply(processor.clean_text).values\n",
    "    \n",
    "    # Convert labels to multi-hot encoding\n",
    "    labels = np.array([processor.process_labels(label) for label in df['label']])\n",
    "    \n",
    "    # Tokenize texts\n",
    "    encodings = []\n",
    "    for abstract in tqdm(abstracts, desc=\"Tokenizing texts\"):\n",
    "        encoding = processor.tokenize_text(abstract)\n",
    "        encodings.append({\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        })\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = COVIDDataset(\n",
    "        texts=abstracts,\n",
    "        labels=labels,\n",
    "        processor=processor\n",
    "    )\n",
    "    \n",
    "    print(f\"Processed {len(dataset)} samples\")\n",
    "    return dataset\n",
    "\n",
    "class COVIDDataset(Dataset):\n",
    "    def __init__(self, texts, labels, processor):\n",
    "        self.texts = texts\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        sequence = self.processor.text_to_sequence(text)\n",
    "        return {\n",
    "            'sequence': sequence,\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c2392-fed7-4acf-95f4-e1b7814f6301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1f68c10-82a6-4767-af5b-5d62b3c50732",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Training Word2Vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 780/780 [00:32<00:00, 23.72it/s]\n",
      "Validation: 100%|██████████| 195/195 [00:05<00:00, 34.54it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Average Training Loss: 0.2901\n",
      "Average Validation Loss: 0.1960\n",
      "Validation Metrics:\n",
      "  Accuracy: 0.6660\n",
      "  F1: 0.8144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 780/780 [00:32<00:00, 23.68it/s]\n",
      "Validation: 100%|██████████| 195/195 [00:05<00:00, 34.40it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Average Training Loss: 0.1847\n",
      "Average Validation Loss: 0.1539\n",
      "Validation Metrics:\n",
      "  Accuracy: 0.7261\n",
      "  F1: 0.8599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 780/780 [00:32<00:00, 23.68it/s]\n",
      "Validation: 100%|██████████| 195/195 [00:05<00:00, 34.53it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Average Training Loss: 0.1476\n",
      "Average Validation Loss: 0.1465\n",
      "Validation Metrics:\n",
      "  Accuracy: 0.7237\n",
      "  F1: 0.8628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 780/780 [00:32<00:00, 23.67it/s]\n",
      "Validation: 100%|██████████| 195/195 [00:05<00:00, 34.49it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Average Training Loss: 0.1182\n",
      "Average Validation Loss: 0.1599\n",
      "Validation Metrics:\n",
      "  Accuracy: 0.7285\n",
      "  F1: 0.8660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 780/780 [00:33<00:00, 23.63it/s]\n",
      "Validation: 100%|██████████| 195/195 [00:05<00:00, 34.54it/s]\n",
      "/lustre/home/rahmanr12/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Average Training Loss: 0.0869\n",
      "Average Validation Loss: 0.1656\n",
      "Validation Metrics:\n",
      "  Accuracy: 0.7145\n",
      "  F1: 0.8628\n",
      "Training time: 194.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180383/543498721.py:148: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(output_dir, 'best_bilstm_model.pt')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 79/79 [00:02<00:00, 33.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-category Metrics:\n",
      "--------------------\n",
      "\n",
      "Case Report:\n",
      "Precision: 0.0458\n",
      "Recall: 0.0387\n",
      "F1-Score: 0.0419\n",
      "Support: None\n",
      "\n",
      "Diagnosis:\n",
      "Precision: 0.2548\n",
      "Recall: 0.2081\n",
      "F1-Score: 0.2291\n",
      "Support: None\n",
      "\n",
      "Epidemic Forecasting:\n",
      "Precision: 0.0167\n",
      "Recall: 0.0120\n",
      "F1-Score: 0.0140\n",
      "Support: None\n",
      "\n",
      "Mechanism:\n",
      "Precision: 0.1820\n",
      "Recall: 0.2067\n",
      "F1-Score: 0.1935\n",
      "Support: None\n",
      "\n",
      "Prevention:\n",
      "Precision: 0.4420\n",
      "Recall: 0.3982\n",
      "F1-Score: 0.4189\n",
      "Support: None\n",
      "\n",
      "Transmission:\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1-Score: 0.0000\n",
      "Support: None\n",
      "\n",
      "Treatment:\n",
      "Precision: 0.3469\n",
      "Recall: 0.4522\n",
      "F1-Score: 0.3926\n",
      "Support: None\n",
      "\n",
      "Overall Metrics:\n",
      "--------------\n",
      "\n",
      "Exact Match Ratio: 0.1716\n",
      "Hamming Accuracy: 0.7306\n",
      "\n",
      "Averaged Metrics:\n",
      "           Micro-avg  Macro-avg  Weighted-avg\n",
      "Metric                                       \n",
      "Precision   0.315805   0.183999      0.303511\n",
      "Recall      0.310694   0.187977      0.310694\n",
      "F1-Score    0.313228   0.184290      0.304391\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'output/bert'\n",
    "ensure_output_dir(output_dir)\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "metrics_tracker = MetricsTracker()\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv('./dataset/BC7-LitCovid-Train.csv')\n",
    "val_df = pd.read_csv('./dataset/BC7-LitCovid-Dev.csv')\n",
    "test_df = pd.read_csv('./dataset/BC7-LitCovid-Test-GS.csv')\n",
    "\n",
    "\n",
    "# Initialize processor\n",
    "processor = DocumentProcessor(embedding_dim=300, max_length=200)\n",
    "processor.get_unique_labels(train_df)\n",
    "\n",
    "# Clean texts\n",
    "train_texts = train_df['abstract'].apply(processor.clean_text).values\n",
    "val_texts = val_df['abstract'].apply(processor.clean_text).values\n",
    "test_texts = test_df['abstract'].apply(processor.clean_text).values\n",
    "\n",
    "# Train Word2Vec and create embeddings\n",
    "print(\"Training Word2Vec model...\")\n",
    "processor.load_word2vec(train_texts)\n",
    "\n",
    "# Process labels\n",
    "train_labels = np.array([processor.process_labels(label) for label in train_df['label']])\n",
    "val_labels = np.array([processor.process_labels(label) for label in val_df['label']])\n",
    "test_labels = np.array([processor.process_labels(label) for label in test_df['label']])\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = COVIDDataset(train_texts, train_labels, processor)\n",
    "val_dataset = COVIDDataset(val_texts, val_labels, processor)\n",
    "test_dataset = COVIDDataset(test_texts, val_labels, processor)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Initialize model\n",
    "model = BiLSTMClassifier(\n",
    "    vocab_size=processor.vocab_size,\n",
    "    embedding_dim=processor.embedding_dim,\n",
    "    num_classes=len(processor.labels)\n",
    ")\n",
    "\n",
    "# Initialize embedding layer with pretrained embeddings\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(processor.embedding_matrix))\n",
    "\n",
    "# Train model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# train_model(model, train_loader, val_loader, num_epochs=5, device=device)\n",
    "\n",
    "total_epoch = 5\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "# Replace the main training section with:\n",
    "for epoch in range(total_epoch):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} Training'):\n",
    "        sequences = batch['sequence'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validation'):\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            val_predictions.extend(outputs.cpu().numpy())\n",
    "            val_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_predictions = np.array(val_predictions)\n",
    "    val_true_labels = np.array(val_true_labels)\n",
    "    val_metrics = calculate_metrics(val_true_labels, val_predictions)\n",
    "    \n",
    "    # Update metrics tracker\n",
    "    metrics_tracker.update({\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'accuracy': val_metrics['exact_match_accuracy'],\n",
    "        'f1': val_metrics['f1']\n",
    "    })\n",
    "    \n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    print(f'Average Training Loss: {avg_train_loss:.4f}')\n",
    "    print(f'Average Validation Loss: {avg_val_loss:.4f}')\n",
    "    print(f'Validation Metrics:')\n",
    "    print(f'  Accuracy: {val_metrics[\"exact_match_accuracy\"]:.4f}')\n",
    "    print(f'  F1: {val_metrics[\"f1\"]:.4f}')\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, 'best_bilstm_model.pt'))\n",
    "\n",
    "training_end_time = time.time()\n",
    "print(f\"Training time: {training_end_time - training_start_time:.2f} seconds\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(metrics_tracker, 1, output_dir)\n",
    "\n",
    "# Final evaluation on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "evaluation_start_time = time.time()\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(output_dir, 'best_bilstm_model.pt')))\n",
    "test_loss, test_predictions, test_labels_array, overall_metrics, per_category_metrics = evaluate_final(\n",
    "    model, test_loader, processor.labels, device\n",
    ")\n",
    "\n",
    "evaluation_end_time = time.time()\n",
    "\n",
    "\n",
    "# Create visualizations\n",
    "# plot_confusion_matrices(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "# plot_roc_curves(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "# plot_metrics_heatmap(per_category_metrics, processor.labels, output_dir)\n",
    "# plot_label_distribution(train_labels, test_predictions, processor.labels, output_dir)\n",
    "\n",
    "# Save detailed performance analysis\n",
    "create_performance_tables(test_labels_array, test_predictions, processor.labels, output_dir)\n",
    "\n",
    "# Save summary metrics\n",
    "with open(os.path.join(output_dir, 'metrics_summary.txt'), 'w') as f:\n",
    "    f.write(\"Overall Metrics:\\n\")\n",
    "    f.write(\"---------------\\n\")\n",
    "    for metric_type, metrics in overall_metrics.items():\n",
    "        f.write(f\"\\n{metric_type}:\\n\")\n",
    "        if isinstance(metrics, dict):\n",
    "            for name, value in metrics.items():\n",
    "                f.write(f\"{name}: {value:.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{metrics:.4f}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c418d3-8b71-4469-a290-4a9e7884ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "335c50d8-a66d-4d24-826a-e5165f99e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time = 194.8161633014679s\n",
      "Evaluation time = 2.4641811847686768s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training time = {(training_end_time-training_start_time)}s\")\n",
    "print(f\"Evaluation time = {(evaluation_end_time-evaluation_start_time)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb60ae5-8e11-4277-aeba-4593624d9284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d674da-f127-4ece-898c-de0b3a4f11c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cc185-d27d-4a94-97ed-e8f1a1d534d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a8f35-613e-4757-9f51-08bc6cab6852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed1fa1-18ee-47b9-9303-6daef966c860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e43948-81bb-4546-81d9-ef23347c90c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
